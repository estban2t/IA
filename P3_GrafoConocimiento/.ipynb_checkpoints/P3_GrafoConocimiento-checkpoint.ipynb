{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3: Grafos de conocimiento\n",
    "#### (y algo de NLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ingeniería Electrónica**\n",
    "\n",
    "**Inteligencia Artificial**\n",
    "\n",
    "**28/04/2021**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El grafo de conocimiento es un concepto fascinante de la ciencia de datos. Es un método de representación del conocimiento mediante entidades interconectadas que pueden ser personas, ubicaciones, eventos, organizaciones, etc. \n",
    "\n",
    "Para construir un grafo de conocimiento, necesitamos formar **triples:sujeto, predicado y objeto** (o *entitad-propiedad-valor*) para vincular datos utilizando ontologías y semántica. \n",
    "\n",
    "En esta práctica, se utilizará una popular librería de Python para el procesamiento avanzado de lenguaje natural, spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías adicionales\n",
    "\n",
    "⚠️ Ejecutar los siguientes comandos en la terminal para instalar las librerías necesarias:\n",
    "\n",
    "   * conda install -c conda-forge spacy\n",
    "   * python -m spacy download en_core_web_sm  (*procesamiento para idioma __inglés__*)\n",
    "   * python -m spacy download es_core_news_sm  (*procesamiento para idioma __español__*)\n",
    "   * pip install bs4 (puede estar ya incluido en anaconda)\n",
    "   * pip install networkx (puede estar ya incluido en anaconda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8faac2276638>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSpan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdisplacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "from spacy import displacy\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisa la documentación de estas librerías para conocer sus funcionalidades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizar entidades y relaciones\n",
    "\n",
    "El procesamiento de texto con `nlp` devuelve un objeto `Doc` que contiene toda la información sobre los *tokens*, sus características lingüísticas y sus relaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm') # para procesamiento de textos en español\n",
    "\n",
    "doc = nlp(\"El tipo lanzó el ladrillo\")\n",
    "\n",
    "for tok in doc:\n",
    "  print(tok.text, \"...\", tok.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para visualizar en un cuaderno de Jupyter, se usa `displacy.render`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') # para procesamiento de textos en inglés\n",
    "\n",
    "doc = nlp(\"If your hate could be turned into electricity, it would light up the whole world.\")\n",
    "\n",
    "for tok in doc:\n",
    "  print(tok.text, \"-->\", tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "doc = nlp(\"La imagen del agujero negro fue renderizada por la joven ingeniera.\")\n",
    "\n",
    "for tok in doc:\n",
    "  print(tok.text, \"-->\", tok.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construir un grafo de conocimiento a partir de datos de texto\n",
    "\n",
    "Lo construiremos en base un conjunto de datos, oraciones compiladas de artículos de wikipedia relacionados con la producción de cine. El archivo CSV de ejemplo está incluido en la carpeta de la práctica como \"wiki_sentences.csv\". Éstas oraciones están en inglés por lo que se debe cargar el modelo correspondente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leer Datos desde CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar archivo con oraciones de wikipedia utilizando pandas (pd)\n",
    "sentencias_candidatas = pd.read_csv(\"wiki_sentences.csv\")\n",
    "sentencias_candidatas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una muestra de 5 oraciones\n",
    "sentencias_candidatas['sentence'].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"today, the trend is for more shallow focus\")\n",
    "\n",
    "for tok in doc:\n",
    "  print(tok.text, \"-->\", tok.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de pares de entidades\n",
    "\n",
    "Se pretende analizar una oración para extraer el sujeto y el objeto (las entidades) a medida que es leída. Sin embargo, existen algunos desafíos: una entidad puede abarcar varias palabras, y los analizadores de dependencia etiquetan solo las palabras individuales como sujetos u objetos.\n",
    "\n",
    "La siguiente función extrae las entidades de una oración tomando en cuenta los problemas mencionados.\n",
    "\n",
    "**Parte 1**\n",
    "En la primera porción de código, se han definifo variables vacías. **prv_tok_dep** y **prv_tok_text** contendrán la etiqueta de dependencia de la palabra anterior en la oración y la palabra anterior en sí, respectivamente. **prefix** y **modifier** contendrán el texto asociado con el sujeto o el objeto.\n",
    "\n",
    "**Parte 2**\n",
    "A continuación, recorreremos los tokens en la oración. Primero verificaremos si el token es un signo de puntuación o no. En caso de serlo, lo ignoraremos y pasaremos al siguiente token. Si el token es parte de una palabra compuesta (etiqueta de dependencia = \"*compound*\"), lo mantendremos en la variable *prefix*. Una palabra compuesta es una combinación de varias palabras vinculadas para formar una palabra con un nuevo significado (ejemplo: \"pista de patinaje\", \"amante de los libros\").\n",
    "\n",
    "**Parte 3**\n",
    "Si el token es el sujeto, se capturará como la primera entidad en la variable **ent1**. Las variables prefix, modifier, prv_tok_dep y prv_tok_text se restablecen.\n",
    "\n",
    "**Parte 4**\n",
    "Si el token es el objeto, se capturará como la segunda entidad en la variable **ent2**. Las variables prefix, modifier, prv_tok_dep y prv_tok_text se restablecen.\n",
    "\n",
    "**Parte 5**\n",
    "Una vez que se haya capturado el sujeto y el objeto en la oración, actualizaremos el token anterior y su etiqueta de dependencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sent):\n",
    "  ## Parte 1\n",
    "  ent1 = \"\"\n",
    "  ent2 = \"\"\n",
    "\n",
    "  prv_tok_dep = \"\"    # etiqueta de dependencia del token anterior en la oración\n",
    "  prv_tok_text = \"\"   # token anterior en la oración\n",
    "\n",
    "  prefix = \"\"\n",
    "  modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "  for tok in nlp(sent):\n",
    "    ## Parte 2\n",
    "    # Si el token es un signo de puntuación, pase al siguiente token\n",
    "    if tok.dep_ != \"punct\":\n",
    "      # comprobar: token es una palabra compuesta o no\n",
    "      if tok.dep_ == \"compound\":\n",
    "        prefix = tok.text\n",
    "        # si la palabra anterior también era un 'compound', entonces agregar la palabra actual\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      # comprobar: el token es un modificador o no\n",
    "      if tok.dep_.endswith(\"mod\") == True:\n",
    "        modifier = tok.text\n",
    "        # si la palabra anterior también era un 'compound', entonces agregar la palabra actual\n",
    "        if prv_tok_dep == \"compound\":\n",
    "          modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "      ## Parte 3\n",
    "      if tok.dep_.find(\"subj\") == True:\n",
    "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "        prefix = \"\"\n",
    "        modifier = \"\"\n",
    "        prv_tok_dep = \"\"\n",
    "        prv_tok_text = \"\"      \n",
    "\n",
    "      ## Parte 4\n",
    "      if tok.dep_.find(\"obj\") == True:\n",
    "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "      ## Parte 5 \n",
    "      # actualizar variables\n",
    "      prv_tok_dep = tok.dep_\n",
    "      prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "  return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos esta función en una oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(\"An electric motor uses electrical energy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la oración anterior, \"*electric  motor*\" es el sujeto y \"*electrical energy*\" es el objeto.\n",
    "\n",
    "Ahora podemos usar esta función para extraer estos pares de entidades para todas las oraciones en nuestros datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pares_entidades = []\n",
    "\n",
    "for i in tqdm(sentencias_candidatas[\"sentence\"]): # con tqdm podemos mostrar una barra de progreso\n",
    "  pares_entidades.append(get_entities(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lista **pares_entidades** contiene todos los pares sujeto-objeto de las oraciones del archivo csv. Revisemos a algunos de ellos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pares_entidades[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extracción de relaciones / predicados\n",
    "\n",
    "El predicado puede ser el verbo principal de una oración. La siguiente función utiliza la concordancia basada en reglas de spaCy para encontrar dichos predicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation(sent):\n",
    "\n",
    "  doc = nlp(sent)\n",
    "\n",
    "  # objeto de la clase Matcher\n",
    "  matcher = Matcher(nlp.vocab)\n",
    "\n",
    "  # definir el patrón \n",
    "  pattern = [[{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}]] \n",
    "    \n",
    "  matcher.add(\"matching_1\", pattern) \n",
    "\n",
    "  matches = matcher(doc)\n",
    "  k = len(matches) - 1\n",
    "\n",
    "  span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "  return(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El patrón definido en la función intenta encontrar la palabra raíz (ROOT) o el verbo principal en la oración. Una vez que se identifica el ROOT, el patrón verifica si es seguido por una preposición (\"prep\") o una palabra de agente. De ser así, se agrega a la palabra ROOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relation(\"An ECG detects the heartbeats.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del mismo modo, se pueden obtener las relaciones de todas nuestras oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relaciones = [get_relation(i) for i in tqdm(sentencias_candidatas['sentence'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisamos a las relaciones o predicados más frecuentes que se acaban de extraer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(relaciones).value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta que relaciones como \"A es B\" y \"A era B\" son las relaciones más comunes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir un grafo de conocimiento\n",
    "\n",
    "Finalmente crearemos un grafoo de conocimiento a partir de las entidades extraídas (pares sujeto-objeto) y los predicados (relación entre entidades). Creamos un dataframe de entidades y predicados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraer sujeto\n",
    "fuente = [i[0] for i in pares_entidades]\n",
    "\n",
    "# extraer objeto\n",
    "objetivo = [i[1] for i in pares_entidades]\n",
    "\n",
    "gc_df = pd.DataFrame({'fuente':fuente, 'objetivo':objetivo, 'arista':relaciones})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, utilizaremos la librería **networkx** para crear una red a partir de este dataframe. Los nodos representarán las entidades y las aristas o conexiones entre los nodos representarán las relaciones entre los nodos.\n",
    "\n",
    "Será un grafo dirigido. En otras palabras, la relación entre cualquier par de nodos conectados no es bidireccional, es solo de un nodo a otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear un grafo dirigido desde un dataframe\n",
    "G=nx.from_pandas_edgelist(gc_df, \"fuente\", \"objetivo\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graficamos la red (esto puede tardar unos minutos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulta que hemos creado un gráfico con todas las relaciones que teníamos. Se vuelve realmente difícil visualizar un gráfico con muchas relaciones o predicados.\n",
    "\n",
    "Por lo tanto, es recomendable utilizar solo unas pocas relaciones importantes para visualizar un gráfico, tomando una relación a la vez. Por ejemplo, con la relación \"composed by\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(gc_df[gc_df['arista']==\"composed by\"], \"fuente\", \"objetivo\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5) # k regula la distancia entre nodos\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esa es una gráfica mucho más legible. Aquí las flechas apuntan hacia los compositores. Veamos algunas relaciones más. Qué se puede notar en los resultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(gc_df[gc_df['arista']==\"written by\"], \"fuente\", \"objetivo\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.from_pandas_edgelist(gc_df[gc_df['arista']==\"released in\"], \"fuente\", \"objetivo\", \n",
    "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "pos = nx.spring_layout(G, k = 0.5)\n",
    "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este práctica, se revisó cómo extraer información de un texto dado en forma de triples (ontologías) y construir un gráfico de conocimiento a partir de él.\n",
    "\n",
    "Sin embargo, nos limitamos a usar oraciones con exactamente 2 entidades. Incluso así pudimos construir gráficos de conocimiento bastante informativos. Consideren el potencial que tenemos aquí.\n",
    "\n",
    "Se podría explorar más este campo de extracción de información para realizar la extracción de relaciones más complejas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
